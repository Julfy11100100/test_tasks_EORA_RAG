# Тестовый проект по обработке и поиску информации

Данный репозиторий содержит тестовый проект, разработанный в рамках тестового задания.

## Алгоритм работы

### 1. Парсинг источников  
Проект начинается с обработки списка ссылок. Для сбора информации используется собственный парсер `HTMLParser`.

### 2. Формирование семантических чанков  
Собранные данные разбиваются на текстовые сегменты с помощью модуля `ContentChunker` для более удобной обработки.

### 3. Векторизация и сохранение  
Каждый текстовый сегмент преобразуется в векторное представление (embedding) через сервис `EmbeddingService` и добавляется в базу данных векторных представлений `ChromaDB`.

### 4. Сервер для обработки пользовательских запросов  
Реализовано web-приложение на FastAPI, принимающее вопросы пользователей, связанные с содержимым сайтов.

### 5. Поиск релевантной информации и генерация ответов  
С помощью `SearchService` определяется наиболее релевантная информация из базы. Модуль `ResponseGenerator` формирует на её основе развернутый ответ. Для генерации используются две языковые модели (`LLM`): `sonar` и `GigaChat`. Выбор модели осуществляется через параметр `LLM_PROVIDER` в `config.py`.

---

## Технологии и компоненты

- Python
- FastAPI
- BeautifulSoup
- Векторные представления с использованием SentenceTransformer и ChromaDB
- LLM-модели: sonar и GigaChat

---

## Запуск проекта

1. Клонируйте репозиторий  
2. Установите зависимости из `requirements.txt`  
3. Создайте `.env` файл на основе `.env.example`. Вставьте свои креды, такие как `GIGACHAT_API_KEY` и `SONAR_API_KEY`  
4. Запустите сервер FastAPI (например, через `uvicorn app.main:app --host 0.0.0.0 --port 8000` либо `app/main.py`)  
5. Отправляйте запросы к API с вопросами по содержимому сайтов на эндпоинт `/ask` (удобнее всего через Swagger — `0.0.0.0:8000/docs`)

---

## Далее информация по тестовому заданию

### Что пробовали сделать?  
Помимо `Sonar` и `GigaChat`, попытался генерировать ответы через другие модели, например — Amvera LLM с моделями `llama8b` и `llama70b`.  
Перепробовал кучу разных промтов, настроек модели, таких как `temperature`, `max_tokens`.  

### Что сработало, а что не очень?  
В ходе работы с `GigaChat` выяснилось, что у `GigaChat API` очень строгий фильтр цензуры, и промты, которые срабатывали на сайте, по API выдавали стандартный ответ по типу:  

> "Генеративные языковые модели не обладают собственным мнением – их ответы являются обобщением информации, находящейся в открытом доступе. Чтобы избежать ошибок и неправильного толкования, разговоры на чувствительные темы могут быть ограничены."

В итоге остановился на `Sonar` (благо нашел аккаунт с Perplexity Pro, и у них есть 5$ со старта на токены). Но и тут пришлось повозиться с промтами и настройками. Основная проблема была в том, что модель, несмотря на прописанные запреты, находила какие-то ссылки вне контекста.

### Как оценили качество решения?  
На мой взгляд — поставленная задача выполнена. Возможно, упустил какие-то детали, так как ранее не работал с RAG.

### Что бы ещё добавили в решение, если бы было больше времени?  
- Реализовать Telegram-бота.  
- Доработать парсер.  
- Попробовать другие LLM модели.  
- Покрыть код тестами.

---
